# The Tragedy of Pangu: The Bittersweet and Dark Journey of Huawei Noah’s Pangu Large Model Development

[汉语原文](README.md)

Hello everyone,

I am an employee of the Pangu large model team at Huawei’s Noah’s Ark Laboratory.

To verify my identity, here are some details:

1. The current Noah director is Wang Yunhe, previously the head of the Algorithm Application Department, later renamed the Small Model Laboratory. The former Noah director was Yao Jun (known as Teacher Yao). Other lab directors include Tang Ruiming (Brother Ming, Captain Ming, now resigned), Shang Lifeng, Zhang Wei (Brother Wei), Hao Jianye (Teacher Hao), Liu Wulong (referred to as Director Wulong), among others. Many key members and experts have gradually left.
2. We are part of an organization called “Four Fields.” Under Four Fields, there are many brigades, with the foundational language large model being the Fourth Brigade. Wang Yunhe’s small model team is the Sixteenth Brigade. We participated in the Suzhou gatherings, which had various monthly deadlines. During the Suzhou gatherings, task orders were issued, requiring us to meet targets by specific deadlines. Personnel from various locations were centralized at the Suzhou Research Institute, usually staying in hotels, such as one in Luzhi, far from family and children.
3. During the Suzhou gatherings, working on Saturdays was the default, and it was exhausting, though we had afternoon tea on Saturdays, and once even had crayfish. The workstations at the Suzhou Research Institute were relocated once, moving from one building to another. The institute’s buildings are decorated in a European style with a large slope at the entrance and beautiful scenery inside. A Suzhou gathering typically lasted at least a week, sometimes longer, with some staying one or two months without returning home.
4. Noah was once rumored to be research-oriented, but after joining the large model project under Four Fields, team members became entirely delivery-focused, filled with routine meetings, reviews, and reports. Experiments often required applications. The team had to coordinate with multiple business lines, including Xiaoyi (terminal), Huawei Cloud, and ICT, facing significant delivery pressure.
5. The Pangu model developed by Noah was initially codenamed “Pangu Zhizi” internally, with only a web version available for internal trial applications. Later, due to pressure, it was integrated into WeLink and opened for public testing.

Recently, the controversy over allegations that the Pangu large model plagiarized Qwen has caused an uproar. As a member of the Pangu team, I’ve been unable to sleep, tossing and turning every night. The Pangu brand has been severely impacted. On one hand, selfishly, I worry about my career and feel that my past hard work has been in vain. On the other hand, I feel a sense of vindication as some have begun exposing these issues. For countless days and nights, we gritted our teeth in frustration, powerless against certain individuals within the company who repeatedly profited through falsification. This suppression and humiliation have gradually eroded my attachment to Huawei, leaving me in a daze, confused, and often questioning my life and self-worth.

I admit I’m a coward. As a mere worker, I dare not confront powerful figures like Wang Yunhe or take on a giant like Huawei. I fear losing my job, as I have a family and children to support, so I deeply admire those who have spoken out. However, seeing the internal attempts to cover up the truth and deceive the public, I can no longer tolerate it. I want to be brave for once and follow my conscience. Even if it costs me greatly, I hope to deal a blow to the wrongdoers. I’ve decided to share what I’ve seen and heard (some from colleagues’ accounts) about the “legendary story” of the Pangu large model:

Huawei primarily trained large models on Ascend chips (though the Small Model Laboratory had many NVIDIA cards, which they used initially before switching to Ascend). I was once inspired by Huawei’s determination to “build the world’s second choice,” and I had deep affection for the company. We worked tirelessly with Ascend, evolving from a buggy platform to one capable of training models, at great cost and effort.

Initially, our computing power was limited, training models on 910A, which only supported fp16, with stability far inferior to bf16. Pangu’s MoE (Mixture of Experts) started early, focusing on a 38B MoE model in 2023 and later a 71B dense model. The 71B dense model was scaled up to become the first-generation 135B dense model, with subsequent main models trained on 910B.

Both the 71B and 135B models had a significant flaw: the tokenizer. The tokenizer used at the time had extremely low encoding efficiency, with each symbol, number, space, or Chinese character occupying a single token. This wasted computing power and led to poor model performance. At that time, the Small Model Laboratory had trained its own vocabulary. Yao Jun suspected the tokenizer was the issue (a suspicion later proven correct) and decided to switch tokenizers for the 71B and 135B models, as the Small Model Laboratory had previously experimented with this. The team stitched together two tokenizers and began the replacement process. The 71B model’s replacement failed, while the 135B, using a more refined embedding initialization strategy and retraining on at least 1T of data, successfully switched tokenizers, though the results were still suboptimal.

Meanwhile, other domestic companies like Alibaba and xAI were training on GPUs and had mastered the correct methods, widening the gap with Pangu. An internally trained 230B dense model failed for various reasons, pushing the project to the brink. Facing deadline pressures and strong internal skepticism about Pangu, team morale hit rock bottom. With extremely limited computing power, the team made numerous efforts and struggled. For instance, they discovered the 38B MoE didn’t deliver the expected MoE performance, so they reverted it to a 13B dense model. Since the 38B MoE originated from the early Pangu Alpha 13B with a relatively outdated architecture, the team made adjustments like switching from absolute position encoding to RoPE, removing bias, and adopting RMSNorm. Based on the tokenizer failures and vocabulary replacement experience, this model adopted the vocabulary used by Wang Yunhe’s Small Model Laboratory’s 7B model. The 13B model was later scaled up and retrained to become the second-generation 38B dense model (the main mid-tier Pangu model for months), which was somewhat competitive. However, the larger 135B model, with its outdated architecture and significant damage from the vocabulary replacement (later analysis revealed the stitched vocabulary had severe bugs), lagged far behind leading domestic models like Qwen after retraining. With growing internal skepticism and leadership pressure, the team was in a desperate state.

At this point, Wang Yunhe and his Small Model Laboratory stepped in. They claimed to have transformed the old 135B model, achieving an average 10-point improvement across metrics with just a few hundred billion tokens of training. In reality, this was their first major “shelling” masterpiece applied to a large model. Huawei’s leadership, often laymen overseeing experts, lacked the understanding to see through this nonsense and assumed it was some algorithmic innovation. Internal analysis revealed they had actually retrained Qwen 1.5 110B, adding layers, expanding FFN dimensions, and incorporating mechanisms from Pangu’s Pi paper to reach roughly 135B parameters. The old 135B had 107 layers, while this new model had only 82, with different configurations. The parameter distribution of this mysterious new 135B closely matched Qwen 110B. Even the model code’s class name was “Qwen,” which they didn’t bother to change. This became the so-called 135B V2, which was provided to many downstream applications, including external clients.

This incident deeply shocked those of us who worked honestly. Many internally, including the terminal and Huawei Cloud teams, knew about it and jokingly called it the “Qiangu” (Thousand Ancients) model instead of Pangu. Some team members wanted to report it to BCG, as it was major business fraud, but leadership allegedly suppressed it. Higher-ups, including Yao Jun and possibly Xiong and Cha, were later informed but turned a blind eye, as good results from shelling benefited them. This led several top team members to lose faith, and talk of resignation became common.

At this juncture, Pangu seemed to see a turning point. Since the earlier Pangu models were mostly retrained or modified, Noah lacked expertise in training from scratch, especially on Ascend NPUs. After strong advocacy from core team members, Pangu began training its third-generation model. With immense effort, the team aligned with industry standards in data architecture and training algorithms, efforts entirely unrelated to the Small Model Laboratory.

Initially, the team lacked confidence and started with a 13B model, but its performance was promising. It was later scaled up to become the third-generation 38B, codenamed 38B V3, familiar to many product line colleagues. This model’s tokenizer was extended based on LLaMA’s vocabulary (a common industry practice). Meanwhile, Wang Yunhe’s lab developed another vocabulary (later used in the Pangu series). The two vocabularies were pitted against each other in a competition, with no clear winner. Leadership then decided to standardize on Wang Yunhe’s vocabulary. Consequently, the 135B V3 (publicly known as Pangu Ultra), trained from scratch, adopted this tokenizer, explaining why the two V3 models used different tokenizers.

From the bottom of our hearts, we felt the 135B V3 was the pride of our Fourth Brigade team. It was Huawei’s first truly self-developed, fully trained billion-parameter model, comparable to competitors in 2024. Writing this brings tears to my eyes—it was no easy feat. To ensure stable training, the team conducted extensive experiments and repeatedly rolled back and restarted when gradients became abnormal. This model achieved what technical reports later claimed: no loss spikes throughout training. We overcame countless challenges and can swear by our lives and honor that this model’s training was authentic. Countless early mornings, we stayed up for its training. Despite being berated as worthless on internal forums, we persevered through frustration and injustice.

We were genuinely burning our youth to build a domestic computing foundation. Living far from home, we sacrificed family, holidays, health, and leisure, pouring our hearts into this endeavor. Words cannot capture the hardships. At mobilization meetings, slogans like “Pangu will win, Huawei will win” deeply moved us.

Yet, our hard-earned results were often casually taken by the Small Model Laboratory. They took our data and code, demanding we adapt it for one-click operation. We jokingly called them the “Mouse-Clicking Lab.” We toiled, while they reaped the glory. As the saying goes, “You bear the burden so others can enjoy peace.” Under these circumstances, more and more colleagues couldn’t hold on and left. Watching talented colleagues depart one by one, I felt both admiration and sorrow. In this war-like environment, they were more like comrades than colleagues, mentors with much to teach me. Seeing them join teams like ByteDance Seed, DeepSeek, Moonshot AI, Tencent, and Kuaishou, I was genuinely happy for them, escaping this grueling yet tainted place. I still remember a resigned colleague’s words: “Coming here was a stain on my technical career; every day I stay is a waste of life.” Harsh but undeniable. My concerns about my technical skills and inability to adapt to the high-pressure elimination culture of internet companies kept me from leaving, despite wanting to.

Beyond dense models, Pangu later explored MoE. A 224B MoE model was trained initially, while the Small Model Laboratory launched its second major shelling operation (with minor instances like a math model), the widely discussed Pangu Pro MoE 72B. They claimed it was scaled up from their 7B model (even so, this contradicts technical reports, not to mention it was retrained from Qwen 2.5’s 14B). Within days, internal evaluations showed it matching the 38B V3. Many in the AI Systems Lab, tasked with model adaptation, knew of their shelling but were silenced for various reasons. Surprisingly, HonestAGI could detect such similarity, as the model’s retraining to wash parameters consumed enough compute to train a same-tier model from scratch. Colleagues said they used dirty data to erase Qwen’s watermark, creating a unique case for academic model lineage research.

By late 2024 and early 2025, DeepSeek’s V3 and R1 releases, with their stunning technical prowess, shocked the team and intensified scrutiny. To keep up, Pangu mimicked DeepSeek’s model size, training a 718B MoE. The Small Model Laboratory struck again, retraining DeepSeek V3 by freezing its loaded parameters. Even the task checkpoint directory was labeled “deepseekv3,” unchanged—how brazen! Meanwhile, colleagues with true technical faith trained another 718B MoE from scratch, facing numerous issues. Clearly, it couldn’t outperform the shelled model. Without the team leader’s insistence, it would have been halted.

Huawei’s cumbersome process management severely slowed large model development, with version control, model lineage, and traceability requirements. Ironically, the Small Model Laboratory’s models seemed exempt, shelling and retraining at will with endless compute resources. This stark, almost surreal contrast highlights the state of process management: “Only the officials may light fires; the commoners may not.” How laughable, tragic, despicable, and shameful!

After the HonestAGI issue surfaced, internal discussions focused on PR and “responses.” Admittedly, the original analysis may lack strength, giving Wang Yunhe and his lab room to deflect and distort the truth. These past few days, I’ve felt nauseated, constantly questioning my life’s meaning and the blindness of justice. I’m done. I’m resigning and requesting removal from the author list of Pangu’s technical reports. Being listed there is a lifelong stain. I never imagined they’d be brazen enough to open-source it or deceive the world so blatantly. Perhaps I clung to a fluke and didn’t refuse authorship. I believe many diligent colleagues were either coerced or unaware. This is irreversible, but I hope to spend my life doing meaningful work to atone for my past weakness.

Writing this late at night, I’m in tears, sobbing uncontrollably. I recall asking departing colleagues with a bitter smile if they’d post a long internal forum thread exposing the truth. They said, “No, it’s a waste of time, and I fear it’d make things worse for you.” I was crestfallen, as comrades who once fought for ideals had lost all faith in Huawei. We joked that we fought with “millet and rifles” like the old Communist Party, but the organization had the style of the old Nationalists.

Once, I was proud of defeating foreign tech with our “millet and rifles.”

Now, I’m tired. I want to surrender.

Even now, I sincerely hope Huawei learns from this, improves Pangu, makes it world-class, and elevates Ascend to NVIDIA’s level. The internal “bad money driving out good” has caused Noah and Huawei to lose many talented large model researchers rapidly. They’re now shining at teams like DeepSeek, pursuing their ambitions in the fierce US-China AI race. I often lament that Huawei doesn’t lack talent but fails to retain it. With the right environment, resources, fewer shackles, and less politics, why couldn’t Pangu succeed?

Finally: I swear by my life, integrity, and honor that everything I’ve written is true (to the best of my knowledge). I lack the technical expertise or opportunity for thorough analysis and dare not use internal records as evidence, fearing repercussions for information security. But I believe my former comrades will vouch for me. Huawei colleagues, including those from product lines we served, can verify the countless details in this post against your experiences. You may have been deceived, but these harsh truths won’t be buried. Our struggles shouldn’t be distorted or forgotten.

Writing this, some will surely try to track me down and silence me. The company may try to hush me or hold me accountable. If so, my safety and that of my family may be at risk. For self-protection, I’ll report my safety daily.

If I disappear, let it be said I sacrificed for truth, ideals, and the better development of Huawei and China’s AI and computing power. I’m willing to be buried in the land where I once fought.

Noah, farewell.

July 6, 2025, early morning, written in Shenzhen

---

Hello everyone,

Thank you for your concern and blessings. I’m currently safe, but the company is likely conducting investigations and compiling lists, and the situation remains uncertain.

I’ll add some details to prevent further distortion of the truth.

Regarding the 135B V2, after the Small Model Laboratory swiftly completed the shelling and reaped all its benefits (e.g., task order commendations and timely incentives), they didn’t want to support downstream applications or model iterations, so they passed this hot potato to the Fourth Brigade. Truly cunning, dragging our colleagues into the mess. They provided an outdated model and returned a modified, advanced Qwen. Those who build models know their creations like their own children—don’t treat others as fools. It’s like sending your son out and getting someone else’s child back.

The authorship of Pangu’s reports violated academic norms. For instance, many who made technical contributions to the 135B V3 were excluded due to author limits, causing significant discontent. This model was the crystallization of our wisdom and sweat, the team’s spiritual pillar, keeping many at Noah. The so-called author limits and inclusion of non-contributors (e.g., from the Small Model Laboratory) deeply disheartened us.
